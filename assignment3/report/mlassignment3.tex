%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{tabularx} % For nice tables

%%%% NEW
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


%%%% NEW

% Margins%
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{Machine Learning} % Top left header
\chead{}
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{16pt} % Removes all indentation from paragraphs

\setcounter{secnumdepth}{0} % Removes default section numbers

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{1in}
\textmd{\textbf{Machine Learning - Assignment 3}} \\
%\textmd{Assignment 2 - Resubmission} \\
\author{Christoffer Thrys√∏e - dfv107}
}

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle
% \tableofcontents
\pagenumbering{arabic}
\section{1. Summarization by the mean}
If we consider a set $S = \lbrace \mathbf{x}_1,...,\mathbf{x}_N \rbrace $, we want to prove that:
\begin{equation}
\textsl{argmin}_{b \in \mathbb{R}^d} \dfrac{1}{N} \sum\limits_{i=1}^N ||\mathbf{x}_i-\mathbf{b}||^2
\label{eq:eq1}
\end{equation} 
is given by the empirical mean:
\begin{equation}
\mathbf{b} = \bar{\mathbf{x}} = \dfrac{1}{N} \sum\limits_{i=1}^N \mathbf{x_i}
\label{eq:eq2}
\end{equation}
First we write out the inner term:
\begin{equation*}
||\mathbf{x_i}-\mathbf{b}||^2 = \left( \sqrt{\mathbf{x}_i^2-\mathbf{b}^2} \right) ^2 = \left(\mathbf{x}_i - \mathbf{b} \right)^2
\end{equation*}
Since we want to find the value of $\mathbf{b}$ which minimizes equation \ref{eq:eq1}, we differentiate with respect to $\mathbf{b}$ and set the equation equal to zero:
\begin{align*}
0 &= \frac{\partial}{\partial \mathbf{b}} \left[
\dfrac{1}{N} \sum\limits_{i=1}^N \left(\mathbf{x_i}-\mathbf{b}\right)^2
\right] \Rightarrow \\
0 &= \dfrac{1}{N} \sum\limits_{i=1}^N -2 \left(\mathbf{x_i}-\mathbf{b}\right) \Rightarrow \\
0 &= \dfrac{1}{N} \sum\limits_{i=1}^N -2\mathbf{x_i} + 2\mathbf{b} \Rightarrow \\
\dfrac{2\mathbf{b}}{2} &= \dfrac{\dfrac{1}{N} \sum\limits_{i=1}^N 2\mathbf{x_i}}{2} \Rightarrow \\
\mathbf{b} &= \dfrac{1}{N} \sum\limits_{i=1}^N \mathbf{x_i}
\end{align*}
Thus completing the proof that equation \ref{eq:eq1} is given by the empirical mean, showed in equation \ref{eq:eq2}.
\section{2. PCA for high dimensional data and small
samples}
The covariance matrix is defined as followed:
\begin{equation}
\mathbf{S} = \mathbf{X_0^T} \mathbf{X_0}
\end{equation}
we want to show that if $\mathbf{v}$ is an eigenvector of $ \mathbf{X}_0 \mathbf{X}_0^T$ with eigenvalue $\lambda$ then $\mathbf{X}_0^T\mathbf{v}$ is an eigenvector of $\mathbf{S}$ with eigenvalue $\lambda$. \\
Given the above we can write the relationship between the matrix, eigenvalue and eigenvector as
\begin{equation}
\mathbf{X}_0 \mathbf{X}_0^T \mathbf{v} = \lambda \mathbf{v}
\label{eq:3}
\end{equation}
If we multiply $X_0^T$ from the left, and noting the definition of $\mathbf{S}$ we get the following:
\begin{align}
\mathbf{X}_0^T \mathbf{X}_0 \mathbf{X}_0^T \mathbf{v} &= \mathbf{X}_0^T \lambda \mathbf{v} \Rightarrow \\
\mathbf{S} \mathbf{X}_0^T \mathbf{v} &= \lambda \mathbf{X}_0^T  \mathbf{v}
\label{eq:4}
\end{align}
comparing equation \ref{eq:3} and \ref{eq:4} $\mathbf{X}_0^T \mathbf{v}$ is the eigenvector of $\textbf{S}$ with eigenvalues $\lambda$, thus completing the proof.
\section{3. The Traffic Sign Recognition Data}
The implementations of the following excercises have been wrapped in a file called \texttt{main.py}. Calling the function will give the component to which 90 percent of the variance is explained, and save plots for figures \ref{fig:hist} \ref{fig:eig} \& \ref{fig:clust1}.
test
\subsection{3.1 Data understanding and preprocessing}
The plotting function is located in the file \texttt{understanding.py}.The program works by splitting the dataset into features, i.e. pictures consisting of 1568 pixel values, and target values, i.e. traffic sign class. The number of each traffic sign class is then accumulated and divided by the total number of signs, resulting in the frequency of each traffic sign. Figure \ref{fig:hist} is a histogram showing the frequency of each class, i.e. traffic sign.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{hist.png}
  \caption{Histogram showing the frequency of each traffic sign from the dataset \texttt{ML2016TrafficSignsTrain.csv}}
  \label{fig:hist}
\end{figure}
\subsection{3.2. Principal component analysis}
The implementation of the PCA algorithm is located in the file \texttt{PCA.py}. The algorithm performs the following steps:
\begin{enumerate}
\item{Separate features and target values of each traffic sign}
\item{For each feature, calculate the mean and withdraw it from the data points}
\item{Compute the covariance matrix of the mean-less data}
\item{Compute eigenvalues and eigenvectors of the covariance matrix}
\item{Sort the eigenvectors based on the corresponding eigenvalue from largest to smallest}
\item{Take the top k (dimension to reduce to) eigenvectors. These correspond to the k principle components}
\item{Project the data into k dimensions by multiplying the principle components and the mean-less data}
\end{enumerate}
\subsubsection{Eigenspectrum}
To compute the eigenspectrum the eigenvalues have been listed in descending order, and plotted on a logarithmic scale as shown in figure \ref{fig:eig}.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{eig.png}
  \caption{Histogram showing the frequency of each traffic sign from the dataset \texttt{ML2016TrafficSignsTrain.csv}}
  \label{fig:eig}
\end{figure}
The number of components necessary to explain 90 $\%$ of the data was determined to be 228 components. \\
From figure \ref{fig:eig} it is evident that the variance drops quickly after a few components, then slowly decreases and for the last 200-300 components the variance plummets.
\subsubsection{Projection onto two dimensions}
The data has been projected down to two dimensions, by performing PCA with k = 2, that is multiplying the first two principle components onto the mean-less data. Figure \ref{fig:scatter} shows a scatter plot for the for the five different mappings from classes to shapes of the traffic signs.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{scatter.png}
  \caption{Scatter plot of the data \texttt{ML2016TrafficSignsTrain.csv} projected onto the two first principal components. each data marker defines a traffic sign shape}
  \label{fig:scatter}
\end{figure}
The below table shows the correlation between the shape of the traffic signs and the colour which they were plotted in. The shape of the marker corresponds to the shape of the traffic sign:
\begin{table}[H]
  \centering
  \begin{tabular}{c||c}
    \textsf{Shape} & \textsf{Color}\\
    \hline
    \textsl{Round} & \textsl{Red} \\
    \textsl{Upwards pointing triangle} & \textsl{Green} \\
    \textsl{Diamond} & \textsl{Blue} \\
    \textsl{Downwards pointing triangle} & \textsl{Yellow} \\
    \textsl{Octagon} & \textsl{Black} \\
  \end{tabular}
\end{table}
From figure \ref{fig:scatter} we can see that the projection onto the two first principal components, describes the data well, as the data is somewhat divided into four clusters. Two classes are not well differentiated, using the two first components, namely the round and octagon shaped signs. This is probably as the round and octagon shapes are very similar, and therefore described similar by the two principal components.

\subsection{3.3. Clustering}
The implementation of k-means clustering is included in the file \texttt{kmeans.py}. The algorithm works as followed:
\begin{enumerate}
\item{$k$ cluster points are initialized, with a given position in the data. For this assignment the data is initialized to have the same position as the first four data points.}
\item{For each data point in the input data, the point is assigned the cluster with the smallest euclidean distance.}
\item{For each cluster, the mean of the assigned points are calculated
and the cluster position is then updated with the mean.
}
\item{The two steps above are repeated until the clusters no longer move. The position of these clusters are then returned. (For this implementation the algorithm stops once the clusters doesn't move, although as the clusters may converge for infinity, this is bad practice, but i didn't experience this.}
\end{enumerate}
Figure \ref{fig:clust1} shows the cluster position projected onto the two first principle components, using the same projection calculation as assignment 3.2. The starting positions of the four clusters correspond to the first four data points. All clusters converged after 20 iterations.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{clust1.png}
  \caption{The result of running 4-clustering on the dataset \texttt{ML2016TrafficSignsTrain.csv}, where the position of the clusters (green dots) have been projected onto the first two principle components.
  }
  \label{fig:clust1}
\end{figure}
I also tried to perform the clustering on random data points. Figure \ref{fig:clust2}, shows the result of doing so.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{clust2.png}
  \caption{The result of running 4-clustering on the dataset \texttt{ML2016TrafficSignsTrain.csv}, with random starting points for the clusters.
  }
  \label{fig:clust2}
\end{figure}
In general I believe the clusters favoured the position corresponding to the rounded and upward triangle shaped traffic signs (red \& green). Also more often than not, a cluster is positioned on the downward triangle shape (yellow).  These observations could be the result that the shapes are represented by a lot of observations, thus influencing the mean greatly and thus dictating the cluster position, but also  that these shapes are well divided into clusters and could therefore easily be grouped, therefore I believe i got meaningful clusters.
\end{document}