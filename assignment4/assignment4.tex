%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{float}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{tabularx} % For nice tables

%%%% NEW
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


%%%% NEW

% Margins%
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{AMachine Learning - Assignment 4} % Top left header
\chead{}
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{16pt} % Removes all indentation from paragraphs

\setcounter{secnumdepth}{0} % Removes default section numbers

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{1in}
\textmd{\textbf{Machine Learning - Assignment 4} \\
\author{Christoffer Thrys√∏e - dfv107}}}
%----------------------------------------------------------------------------------------

\begin{document}
\maketitle
% \tableofcontents
\pagenumbering{arabic}
\section{Finite Hypothesis Space}
\subsection{Question 1}
Below is the hypothesis space in the first and second approach:
\begin{enumerate}
\item{In the first approach every possible individual combination of the pair: $ \lbrace age,gender \rbrace$ are considered, where $\textsl{gender} \in \lbrace \textsl{male,female} \rbrace$ \& $\textsl{age} \in \lbrace 0,...,100\rbrace$ }. Each hyptohesis has a binary outcome, therefore the size of above space is:
\begin{equation*}
|\lbrace 0 ,1 \rbrace |^{|\lbrace 0,..,100\rbrace| \times |\lbrace \textsl{male,female} \rbrace|} =
2^{101 \times 2 } = 2^{202} = 6.43 \times 10^{60}
\end{equation*}
\item{
For the second approach, for each tuple $(i,j)$ we chose the set of indices i,j such that : $ 0 \leq i < j \leq 100$. The indices are chosen from 101 possible age values thus we have the set $\binom{101}{2} = 5050$ and for both gender we have the hypothesis size:
\begin{equation*}
\binom{101}{2}^2 = 5050 ^2 = 25502500
\end{equation*}
}
\end{enumerate}
\subsection{Question 2}
To write a high probability bound on $L(h)$ in terms of $\hat{L}(h,S)$ we use the following bound:
\begin{equation*}
Pr \left[
\exists h \in \mathcal{H} : L(h) \geq \hat{L}(h,S) +
\sqrt{ \dfrac{ ln \frac{M}{\delta}}{2n}}
 \textbf{ } \right] \leq \delta
\end{equation*}
Which we can write the complement of for all $h \in \mathcal{H}$:
\begin{equation}
Pr \left[
\forall h \in \mathcal{H} : L(h) \leq \hat{L}(h,S) + \sqrt{ \dfrac{ ln \frac{M}{\delta}}{2n}}
 \textbf{ } \right] \geq 1 - \delta
\label{eq:2}
\end{equation}
Given the bound for $L(h)$ from equation \ref{eq:2}, we can plug in the numbers for the hypothesis space for the two cases:
\begin{enumerate}
\item{
\begin{equation*}
Pr \left[
\forall h \in \mathcal{H} : L(h) \leq \hat{L}(h,S) + \sqrt{ \dfrac{ ln \frac{6.43 \cdot 10^{60}}{\delta}}{2n}}
 \textbf{ } \right] \geq 1 - \delta
\end{equation*}
The complexity is written as $p(h) = \dfrac{1}{M} = \dfrac{1}{6.43 \cdot 10^{60}}$
}
\item{
\begin{equation*}
Pr \left[latex r
\forall h \in \mathcal{H} : L(h) \leq \hat{L}(h,S) + \sqrt{ \dfrac{ ln \frac{25502500}{\delta}}{2n}}
 \textbf{ } \right] \geq 1 - \delta
\end{equation*}
}
The complexity is written as $p(h) = \dfrac{1}{M} = \dfrac{1}{25502500}$
\end{enumerate}
\subsection{Question 3}
% tighter bound
Choosing a limited hypothesis set results in a bigger chance of the empirical loss being equal to the actual loss. However a reduced hypothesis set may also lead to under-fitting of the data, as the set of hypothesis may not represent the true learning algorithm. Likewise a
not restrictive hypothesis may lead to over-fitting. \\ Choosing a range as the hypothesis is not advantageous when dealing with distribution not entered around a mean, for example a single range of age doesn't describe when people are most likely to visit the dentist.
\section{Occam's Razor}
\subsection{Question 1}
First we note that the size of the alphabet is $|\sum| = 27$, given a string of $d$ characters, the possible space must be: $27^d$, for each of these mappings is an output $|\lbrace 0,1 \rbrace |$ therefore we have the hypothesis space (possible mappings): $
2^{27^{d}} $.
Now we can use equation \ref{eq:2} again to derive at high probability bound on $L(h)$ for all $h \in \mathcal{H}$:
\begin{equation*}
Pr \left[
\forall h \in \mathcal{H} : L(h) \leq \hat{L}(h,S) + \sqrt{ \dfrac{ ln \frac{2^{27^{d}}}{\delta}}{2n}}
 \textbf{ } \right] \geq 1 - \delta
\end{equation*}
\subsection{Question 2}
We start by setting $p(h)= 1/2^{d(h)}2^{27^d}$ we therefore have that:
\begin{equation*}
\sum\limits_{h \in \mathbb{H}} p(h) = \sum\limits_{d=0}^\infty \sum\limits_{h \in \mathcal{H}_d} 1/2^{d}2^{27^d} =
\sum\limits_{d=0}^\infty |\mathcal{H}_d| 1/2^{d}2^{27^d} = 
\sum\limits_{d=0}^\infty 1/2^{d} = 1
\end{equation*}
we can apply Occam's Razor and obtain a high probability bound for all $h \in \mathcal{H}$:
\begin{equation*}
Pr \left[
\forall h \in \mathcal{H} : L(h) \leq \hat{L}(h,S) + \sqrt{ \dfrac{ ln \frac{2^{d(h)} 2^{27^{d(h)}}}{\delta}}{2n}}
 \textbf{ } \right] \geq 1 - \delta
\end{equation*}


\subsection{Question 3}
The advantages of choosing a large d is the possibility to describe the data, with more hypothesis, thus avoiding under-fitting. However choosing a large d may also lead to over-fitting of the data.
\section{3. Logistic regression}
\subsection{3.1 Cross-entropy error measure}
\subsubsection{a}
We have the likelihood function:
\begin{align}
\label{eq:like}
Pr\left\{ \, y \, | \, \mathbf{x} \, \right\} =
\begin{cases} 
      h(\mathbf{x}) & \text{for} \;\, y=+1 \\
      1-h(\mathbf{x}) & \text{for} \;\, y=-1
   \end{cases}
\end{align}
and we know the maximum likelihood selects the hypothesis \textit{h}, which maximizes the probability, which is equivalent to minimizing the quantity:
\begin{equation}
\dfrac{1}{N} \sum\limits_{n=1}^N ln \left( \dfrac{1}{Pr\left( \, y \, | \, \mathbf{x} \, \right)} \right)
\label{eq:comp}
\end{equation}
We can rewrite \eqref{eq:like} in terms of indicator variables:
\begin{equation}
Pr\left\{ \, y \, | \, \mathbf{x} \, \right\} = \mathbbm{1}_{y \in \left\{ +1 \right\}}h(\mathbf{x}) + \mathbbm{1}_{y \in \left\{ -1 \right\}}(1-h(\mathbf{x}))
\label{eq:skill}
\end{equation}
rewriting \eqref{eq:comp} in terms of \eqref{eq:skill} we obtain the following:
\begin{align}
\dfrac{1}{N} \sum\limits_{n=1}^N ln \left( \dfrac{1}{Pr\left( \, y \, | \, \mathbf{x} \, \right)} \right) &=
\dfrac{1}{N} \sum\limits_{n=1}^N ln \left(\dfrac{1}{\mathbbm{1}_{y \in \left\{ +1 \right\}}h(\mathbf{x}) + \mathbbm{1}_{y \in \left\{ -1 \right\}}(1-h(\mathbf{x}))} \right) \\
&= 
\dfrac{1}{N} \sum\limits_{n=1}^N  \mathbbm{1}_{y \in \left\{ +1 \right\}} ln \left(\dfrac{1}{h(\mathbf{x})}\right) + \mathbbm{1}_{y \in \left\{ -1 \right\}} ln \left(\dfrac{1}{1-h(\mathbf{x})} \right)
\label{eq:hello}
\end{align}
which concludes the proof.
\subsubsection{b}
We want to prove that minimizing the in-sample error from \textbf{a} is equivalent to minimizing the following in-sample error:
\begin{equation*}
E_{in}(\mathbf{w}) = \dfrac{1}{N} \sum\limits_{n=1}^N ln \left(1+e^{-y_n \mathbf{w}^T \mathbf{x}_n} \right)
\end{equation*}
when $h(x)= \theta(\mathbf{w}^T\mathbf{x})= \dfrac{e^{\mathbf{w}^T\mathbf{x}}}{1+e^{\mathbf{w}^T\mathbf{x}}} =
\dfrac{1}{1+e^{-\mathbf{w}^T\mathbf{x}}}$ \\
If we substitute this definition of $h(x)$ into \eqref{eq:hello} we get the following:
\begin{equation*}
\mathbbm{1}_{y \in \left\{ +1 \right\}} ln \left(\dfrac{1}{\theta(\mathbf{w}^T\mathbf{x})}\right) + \mathbbm{1}_{y \in \left\{ -1 \right\}} ln \left(\dfrac{1}{1-\theta(\mathbf{w}^T\mathbf{x})} \right)
\end{equation*}
From the second term we note that:
$ \theta (-x) = \dfrac{e^{-x}}{1+e^{-x}}= \dfrac{1}{1+e^s} = 1 - \theta(x) $ therefore $1 - \theta(x) = \theta(-x)$ and we can write:
\begin{equation*}
\mathbbm{1}_{y \in \left\{ +1 \right\}} ln \left(\dfrac{1}{\theta(\mathbf{w}^T\mathbf{x})}\right) + \mathbbm{1}_{y \in \left\{ -1 \right\}} ln \left(\dfrac{1}{\theta(-\mathbf{w}^T\mathbf{x})} \right) =
ln \left( \dfrac{1}{\theta(y_i \mathbf{w}^T\mathbf{x})}\right) \\
\end{equation*}
writing out the sigmoid function we get:
\begin{align*}
ln \left( \dfrac{1}{\theta(y_i \mathbf{w}^T\mathbf{x})}\right) &=
ln \left(  \dfrac{1}{\dfrac{1}{1+e^{-y_n \mathbf{w}^T \mathbf{x}_n}} } \right) \\
&= ln \left( 1 + e^{e^{-y_n \mathbf{w}^T \mathbf{x}_n}} \right)
\end{align*}
thus we get the desired in-sample error:
\begin{equation*}
E_{in}(\mathbf{w}) = \dfrac{1}{N} \sum\limits_{n=1}^N ln \left(1+e^{-y_n \mathbf{w}^T \mathbf{x}_n} \right)
\end{equation*}
\section{3.2 Logistic regression loss gradient}
First we note that the in-sample error measure is defined as:
\begin{equation*}
E_{in}(\mathbf{w}) = \dfrac{1}{N} \sum\limits_{n=1}^N ln \left(1+e^{-y_n \mathbf{w}^T \mathbf{x}_n} \right)
\end{equation*}
We determine the gradient of the in-sample loss error measure:
\begin{equation*}
\nabla E_{in}(\mathbf{w}) = \dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{\partial}{\partial \mathbf{w}} \left[ ln \left(1+e^{-y_n\mathbf{w}^T\mathbf{x}_n}\right) \right]
\end{equation*}
If we let $f(x)=ln(x)$ and $g(x) = 1 + e^{-y_n \mathbf{w}^T \mathbf{x_n}}$, we apply the chain rule for gradients which means that we get:
\begin{equation*}
\dfrac{\partial}{\partial \mathbf{w}} \left[ ln \left(1+e^{-y_n\mathbf{w}^T\mathbf{x}_n}\right) \right] = f'(g(\mathbf{w}))\nabla(\mathbf{w})
\end{equation*}
Applying above we have:
\begin{align*}
f'(g(\mathbf{w})) &= \dfrac{1}{1+e^{-y_n\mathbf{w}^T\mathbf{x}_i}} \\
\nabla g(\mathbf{w}) &= \dfrac{\partial}{\partial \mathbf{w}} \left[
1+ e^{-y_n\mathbf{w}^T\mathbf{x}_n} \right] = e^{-y_n \mathbf{w}^T \mathbf{x}_n}  \times (-y_n \mathbf{x}_n) 
\end{align*}
now we can compute $f'(g(\mathbf{w}))\nabla(\mathbf{w})$:
\begin{align*}
f'(g(\mathbf{w}))\nabla(\mathbf{w}) &=
\dfrac{-y_n \mathbf{x}_n e ^{-y_n\mathbf{w}^T\mathbf{x}_n}}
{1+e^{-y_n \mathbf{w}^T \mathbf{x}_n }} \\
&= 
\dfrac{
{-y_n \mathbf{x}_n e^{-y_n\mathbf{w}^T\mathbf{x}_n}} / {e^{-y_n\mathbf{w}^T\mathbf{x}_n}} }
{1+e^{-y_n \mathbf{w}^T \mathbf{x}_n } / {e^{-y_n\mathbf{w}^T\mathbf{x}_n}}} \\
&= \dfrac{-y_n\mathbf{x}_n}{1+e^{y_n\mathbf{w}^T\mathbf{x}_n}}
\end{align*}
Thus we can write:
\begin{equation*}
\nabla E_{in} (\mathbf{w}) = - \dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{y_n\mathbf{x}_n}{1+e^{y_n\mathbf{w}^T\mathbf{x}_n}}
\end{equation*}
It is clear that:
\begin{equation*}
\dfrac{1}{N} \sum\limits_{n=1}^N -y_n\mathbf{x}_n\theta(-y_n\mathbf{w}^T \mathbf{x}_n) = - \dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{y_n\mathbf{x}_n}{1+e^{y_n\mathbf{w}^T\mathbf{x}_n}}
\end{equation*}
If we write out the sigmoid function:
\begin{align*}
\dfrac{1}{N} \sum\limits_{n=1}^N -y_n\mathbf{x}_n\theta(-y_n\mathbf{w}^T \mathbf{x}_n) &= \dfrac{1}{N} \sum\limits_{n=1}^N -y_n\mathbf{x}_n \dfrac{1}{ 1+ e^{ y_n \mathbf{w}^T \mathbf{x}_n}} \\
&=
\dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{-y_n\mathbf{x}_n}{ 1+ e^{ y_n \mathbf{w}^T \mathbf{x}_n}}\\
&=
-\dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{y_n\mathbf{x}_n}{ 1+ e^{ y_n \mathbf{w}^T \mathbf{x}_n}} \\
\end{align*}
Thus completing the proof.
\subsection{3.3 Logistic regression implementation}
The implementation for logistic regression is found in the file \texttt{logRes.py}. The implementation works as followed:
\begin{enumerate}
\item{First the data is handled. The features and target values are separated.The second class from the dataset is removed and the class 0 is changed to -1. The intercept is added for each feature: $\mathbf{x}_0 = 1$}
\item{Next the total number of iterations and the step size $\alpha$ for gradient decent is specified}
\item{ For each iteration the gradient is calculated as:
\begin{equation*}
\mathbf{g}_t = - \dfrac{1}{N} \sum\limits_{n=1}^N
\dfrac{y_n \mathbf{w}^T }
{1+ e^{ y_n \mathbf{w}^T(t)\mathbf{x}_n}}
\end{equation*}
where the direction of the gradient is defined as
$\mathbf{v}_t = -\mathbf{g}_t $
}
\item{ The weights are updated by:
$\mathbf{w}(t+1) = \mathbf{w}(t) + \alpha \mathbf{v}_t$}
\item{After the last iteration the updated weights are returned.}
\end{enumerate}
Once the weights have been obtained the predictive class is calculated as:
\begin{equation*}
h(x) = \theta(\mathbf{w}^T\mathbf{x})
\end{equation*}
where $\theta$ is the sigmoid function. The predictive class is the determined by:
\begin{align*}
h(x) =
\begin{cases} 
      +1 & \text{for} \;\, h(x) \geq 0.5 \\
      -1 & \text{for} \;\, h(x) < 0.5
   \end{cases}
\end{align*}
The learning hypothesis is measured using a zero one less expressed as the empirical error:
\begin{equation*}
\hat{L}(h) =  \dfrac{1}{N}\sum\limits^{N}_{n=1} \ell (h(x_n),y_n)
\end{equation*}
The below table shows the empirical error on the training and test data, that is the weights have been found using the training data and applied to the training and test data.
\begin{table}[H]
  \centering
  \label{tab:table1}
  \begin{tabular}{c|c}
   \textsf{Test} & \textsf{Train} \\
    \hline
    $0.0168$ & $0.076$ \\  
  \end{tabular}
\end{table}
The results from the above table have been generated by running gradient decent for 10000 iterations with a step size $\alpha=0.1$ The weights found with the setting is 
$$ a = [\mathbf{w}_1, \mathbf{w}_2], b = [\mathbf{w}_0]:$$
$$
a = [3.274,-14.386], b = [-13.015] $$
\end{document}

