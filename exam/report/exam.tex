%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum}
\usepackage{tabularx}
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{
  backgroundcolor=\color{white},   % you must add \usepackage{color} or \usepackage{xcolor}
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1} % Line spacing

\pagestyle{fancy}
\lhead{ML exam - 2017} % Top left header
\chead{}
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule
\setlength\parindent{16pt} % Removes all indentation from paragraphs
\setcounter{secnumdepth}{0} % Removes default section numbers
\title{
\vspace{1in}
\textmd{\textbf{Machine Learning Exam}} \\
\author{Christoffer Thrys√∏e - dfv107}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\pagenumbering{arabic}
\section{1. In a galaxy far, far away}
\subsection{Question 1.1}
The variance $\sigma^ 2_{\text{red}}$ of the redshifts detailed in the training data, was calculated using the following:
\begin{equation}
\sigma^2_{\text{red}} = \dfrac{1}{N} \sum\limits_{i=0}^N(X_i-\mu)^2
\end{equation}
where $\mu$ is the mean of the redshifts defined as:
\begin{equation}
\mu = \dfrac{1}{N} \sum\limits_{i=0}^N x_i
\end{equation}
The variance of the dataset \texttt{ML2016SpectroscopicRedshiftsTrain.dt} was measured to be:
$$
\sigma^ 2_{\text{red}} = 0.0106043856253
$$
The mean-squared-error (MSE) is defined as the following:
\begin{equation}
\text{MSE} = \dfrac{1}{N} \sum\limits_{i=0}^N (\hat{y_i} - y_i)^2 
\end{equation}
where $\hat{y}$ denotes the estimated value of the actual target value $y$. 
MSE has been applied on the SDSS predictions on the test data and the actual values. The returned MSE was the following:
$$
MSE = 0.000812319455494
$$
\subsection{Question 1.2}
Linear regression was performed by applying least squares, however the results were very unsatisfying, probably due to the linear dependency in the data. Therefore i have used the weights described in the announcement on Absalon. From these weights i have estimated the target value $\hat{Y}$ using the following:
\begin{equation}
\hat{Y_i} = wx_i+w_0
\end{equation}
The MSE on the training and test data is defined below:
\begin{align*}
\text{MSE}_{\text{training}} &=  0.0018621 \\
\text{MSE}_{\text{test}} &= 0.00186407
\end{align*}
Normalizing the error w.r.t the variance yields the following result:

\begin{align}
\dfrac{\text{MSE}_{\text{training}}}{\sigma^2_{\text{red}}} =
\dfrac{0.0018621}{0.0106043856253} = 0.17559716006
\label{eq:3}
\end{align}
As the least-squared estimator is unbiased, the mean-squared error is the variance of our linear estimator. By comparing the mean-squared-error, and the variance of the data we can see to what extend the estimator captures the variance of the data. For example if the variance of the data is high and the mean-squared-error is small compared, then the estimator has over-fitted the data. Equation \eqref{eq:3} will then be less than 1. If on the other hand the mean-squared-error is very large compared to the variance, then the estimator has under fitted the data. In this case equation \eqref{eq:3} will be greater than 1.
\subsection{Question 1.3}
For the non-linear regression method i have chosen the k-nearest neighbour. I have used the same implementation as in the previous assignment, however for regression instead of classification instead. The process of obtaining the k-nearest neighbours of a sample is the same, the euclidean distance from each point in question is taken to the rest of the points, the top k points with the lowest distance are marked as neighbours.The regression variant doesn't use a voting phase 
on the target values of the k-nearest neighbours, but the mean of their target values instead as shown in equation \eqref{eq:knn}.
\begin{equation}
\hat{y} =  \dfrac{1}{K} \sum\limits_{i=0}^K y(x_i)
\label{eq:knn}
\end{equation}
where $y$ denotes the target value associated with the independent variable $x$. \\
For hyper parameter tuning, i.e. finding the optimal value of $k$, i have used a 5-fold cross-validation. The training set of 5000 samples is split into 5 equally sized partitions, each of these partitions is by turn used as a validation set and the rest as a training set. The k-nearest neighbour algorithm is applied to each of the partitions, and the average MSE error over these folds determines the average error for the given value k. The k with the lowest average error is used as the parameter of the k-nearest neighbour algorithm.
\\
There was a number of reasons for choosing algorithm, ultimately the choice was between k-nearest-neighbours and neural networks. Both algorithms was considered as they usually make good generalizations on data, without making any assumptions about the data. As the dataset is quite big and consists of many features, this is wanted as finding trends in data, which addresses a certain algorithm is hard, due to the dimensionality. I tested my neural two-layer hidden network implementation and the result was sub par compared to the performance of  the k-nearest neighbour. There is although a set of shortcomings that follows the k-nearest neighbour. Although we may not need to make any assumptions on the data, we also assume that each feature contribute equally to the overall estimation, this is however rarely true. The computation time is also very high, when the dataset is big. One last thing which should be accounted for is that each data feature may measure on different scales, which results in certain features contributing more to the overall estimation. To counter this, the training data has been normalized to exhibit zero-mean and unit-variance. This affine mapping obtained by the training data, has also been applied to the training data. \\
The cross validation has been performed for the following values of k:
$$ k \in \left[ 1,3,5,7,9,11,13,15,17,19,21,23,25 \right] $$
The average error over the five folds is illustrated in figure 

\begin{figure}[H]
  \centering
  \includegraphics[width=17cm]{fig/1.png}
  \caption{}
  \label{fig:mulplots}
\end{figure}
The smallest MSE came from the parameter $k=9$, therefore 9 is the hyperparameter chosen. The following shows the result of running the 9-nearest-neighbour algorithm on the training and testing data. When applying k-nearest neighbour on the training data, the closest neighbour will always be itself, this is not the case for the test data, as the points are introduced to a dataset where they don't exists. The results are the following:
\begin{align*}
\text{MSE}_{\text{train}} &=  0.000465310841171 \\
\text{MSE}_{\text{test}} &=  0.000588914605838
\end{align*}
The above results both show an MSE, which is far lower than the liner regression model. Thus the non-linear 9-nearest-neighbour has a better performance %TODO ..
\section{2. Weed}
\subsection{Question 2.1}
For this assignment I have used the logistic regression algorithm implemented in previous assignments. The implementation uses the the steepest decent variant of gradient decent, where the gradient $g$ is updated as the following
\begin{equation}
g = - \dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{y_nx_n}{1+e^{y_nw^Tx_n}}
\end{equation}
and the weights are updated by: 
\begin{equation}
w = w - \epsilon g
\end{equation}
where $\epsilon$ is the learning rate of the gradient decent. The target class is evaluated as the following:
\begin{equation*}
h(x) = \theta(\mathbf{w}^T\mathbf{x})
\end{equation*}
where $\theta$ is the sigmoid function, which estimates the probability of the given class. Since we have a binary decision problem, we can define the decision boundary as the following: 
\begin{align*}
h(x) =
\begin{cases} 
      +1 & \text{for} \;\, h(x) \geq 0.5 \\
      -1 & \text{for} \;\, h(x) < 0.5
   \end{cases}
\end{align*}
For the model, i have used $\epsilon = 0.00001$. The small learning rate was used as values for the data was large, and a larger learning rate caused the gradient to grow large and cause overflow. The model has been trained using 5000 iterations of the steepest descend algorithm. \\
The zero-one-loss is reported on the training and testing data using the model trained using the training data. The zero-one-loss is defined as followed:
\begin{align*}
l(y,\hat{y}) =
\begin{cases} 
      0 & \text{if} \;\, y = \hat{y} \\
      1 & \text{otherwise}
   \end{cases}
\end{align*}
The errors on the training and test data are as followed:
\begin{align*}
l(y,\hat{y})_{train} &= 24\\
l(y,\hat{y})_{test} &= 18
\end{align*}
The empirical loss of the two errors is:
\begin{align*}
L(y,\hat{y})_{training} &= 0.024\\
L(y,\hat{y})_{test} &= 0.0313
\end{align*}
\subsection{Question 2.2}
%TODO define what C and gamma is
% https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine
We wish to determine G, which is the following quantity:
\begin{equation}
G = \left\lbrace \text{min}_{x_j,y_j) \in S \wedge y_i \neq y_j}
\left\lbrace || x_i - x_j || \right\rbrace | (x_i,y_i) \in S \right\rbrace 
\end{equation}
G is calculated by iterating the training data, and creating a list of training data points sorted by closest distance. The sorted list is then iterated, when a training point where $y_i \neq y_j$ is encountered, the distance $||x_i-x_j||$ is accumulated. The estimate $\sigma_{\text{Jaakkola}}$ is given by:
\begin{equation}
\sigma_{\text{Jaakkola}} = \text{median}(G)
\end{equation}
The bandwidth parameter $\gamma$ can be derived by the following:
\begin{align*}
\sigma_{\text{Jaakkola}} &= \sqrt{1 / (2\gamma_{\text{Jaakkola}})} \Rightarrow \\
\gamma_{\text{Jaakkola}} &= 1/2\sigma^2_{\text{Jaakkola}}
\end{align*}
I estimated $\sigma_{\text{Jaakkola}} = 898.205380106$
therefore $\gamma_{\text{Jaakkola}}$ is estimated by:
\begin{equation}
\gamma_{\text{Jaakkola}} = 1 / 2 \times 898.205380106^2 = 6.197 \times 10^{-07}
\end{equation}
For the task, the library LIBSVM for python was used for training and classification. For parameter tuning grid search was applied, where each parameter setting was evaluated by performing 5-fold cross-validation. The choosing of parameters which yielded the lowest average zero - one loss over the 5-folds was inserted into the grid. The grid index with the lowest average zero - one loss, was determined as parameter settings. The values for $C$ and $\gamma$ were picked from the following set:
\begin{align*}
(C,\gamma) \in \left\{ b^{-3}, b^{-2}, b^{-1}, 1, b, b^{2}, b^{3} \right\} \times \left\{ \gamma_{Jaakkola} \cdot b^{i} | i \in \left\{ -3, -2, -1, 0, 1, 2, 3 \right\} \right\}
\end{align*}
where $b \in \lbrace 2, e, 10 \rbrace$.
Running the 5-fold cross validation the parameters for $C$ and $\gamma$ which gave the lowest average zero - one loss was
\begin{align*}
C &= b^4 \\
\gamma &= \gamma_{Jaakkola} \cdot b^0
\end{align*}
The lowest error was measured when b was 2 or $e$. The above parameters was obtained during training, below is the error of applying the SVM, with the above parameters on the training and testing data.

\begin{align*}
L(y,\hat{y})_{\text{train}} &= 0.015 \\
L(y,\hat{y})_{\text{test}} &= 0.0348
\end{align*}
\subsection{Question 2.3}
%TODO how was zero mean and unit variance achieved
To achieve zero-mean and unit-variance, we apply a linear mapping from the input data to the normalized output data, that is of the form $R^d \rightarrow R^d$. Thus we wish to determine some normalization function $ f_{a,b}$:
\begin{equation}
X_{f_{a,b}}  = \lbrace a x_i+b | x_i \in X \rbrace
\end{equation}
such that $\mu(X_{f_{a,b}})=0$ and $Var(X_{f_{a,b}})=1$ \\
To achieve this, the mean of each input feature is found and subtracted. The mean is calculated by:
\begin{equation*}
\mu(X) = \dfrac{1}{N} \sum\limits^{N}_{i=1}x_i
\end{equation*}
 Next the variance is found of each feature. The variance is calculated by:
\begin{equation*}
Var(X) = \dfrac{1}{N} \sum\limits^{N}_{i=1}(x_i - \mu)^2
\end{equation*}
The standard deviation $\sigma$ is equal to the square root of the variance and is used to normalize the data. The normalization can then be expressed as the following:
\begin{equation}
X_{f_{a,b}} = \dfrac{1}{\sigma_X} + \dfrac{- \mu_X}{\sigma_X} = \dfrac{X - \mu_X }{\sigma_X}
\label{norm}
\end{equation}
For the below algorithms i will first state how i predict the normalization will effect the estimation, then results of the algorithms on the normalized data.
\subsubsection{Logistic regression}
First i will determine how the estimator is affected by the normalization. Given a logistic regression applied to normalized data we include the affine transformation in the calculation:
\begin{align*}
w^Tx' &= w^T \cdot (a x_i +b) \\
&= w_0 + \sum\limits_{i=0}^N w \cdot (ax_i+b) \\ 
&= w'_0 + \sum\limits_{i=1}^N w x_i \\
&= w'^Tx
\end{align*}
when $x_0 = w_0 + \sum_{i=0}^d b_i w_i$, $ w'_i = a w_i$. Therefore the weights obtained using normalized data, can be obtained by using the affine transformation parameters. Therefore the normalization shouldn't effect the classification procedure. \\
Logistic regression uses gradient decent and this procedure, however effects from data normalization. One can view the normalization process of normalizing the shape of the gradient surface. Normalization is often  used in the context of optimization problems, where computations of large gradients are expensive and may cause overflow. The normalization also makes convergence faster and ensures that weights are updated somewhat equally.



Applying normalization to the data before performing logistic regression worsens the generalazation in contrast to the SMV. Below is the average zero - one loss when normalization is applied to the dataset:
\begin{align*}
L(y,\hat{y})_{\text{trainnorm}} &= 0.440 \\
L(y,\hat{y})_{\text{testnorm}} &= 0.433
\end{align*}
As evident from the above errors, the logistic regression estimator is now only capabale of correctly classifying a little of half of the training and test data, which is very bad.
\subsubsection{Support vector machines}
Achieving zero-mean and unit-variance when using support vector machines is useful in multiple ways. kernels often depend on distances between points e.g the Gaussian kernel. To avoid inconsistency when calculating these distances it's necessary to scale the features to the same range. Like with logistic regression, SVM's solve an optimization problem, which benefits from scaled features.
%
%http://sebastianraschka.com/Articles/2014_about_feature_scaling.html
%The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Another advantage is to avoid numerical diculties during the calculation. Because kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial ker- nel, large attribute values might cause numerical problems. We recommend linearly scaling each attribute to the range [-1,+1] or [0,1].

Applying zero mean and unit variance to the dataset before running the training the support vector machine leads to a different loss and different choice of hyper parameters. $b=2$ is still the best setting for b. The found parameters using grid search are as followed:
\begin{align*}
C &= b^3 \\
\gamma &= \gamma_{Jaakkola} \cdot b^{-3}
\end{align*}
The linear mapping was achieved on the training set, the same mapping is used on test set. The training and test error is as followed:
\begin{align*}
L(y,\hat{y})_{\text{trainnorm}} &= 0.015 \\
L(y,\hat{y})_{\text{testnorm}} &= 0.0278
\end{align*}
We note from the above errors that the normalized data yields the same train error, however we achieve a much better generalization of the data as the test error is lower.
%TODO discussion on why


\end{document}
