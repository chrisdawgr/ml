%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum}
\usepackage{tabularx}
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{
  backgroundcolor=\color{white},   % you must add \usepackage{color} or \usepackage{xcolor}
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1} % Line spacing

\pagestyle{fancy}
\lhead{ML exam - 2017} % Top left header
\chead{}
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule
\setlength\parindent{16pt} % Removes all indentation from paragraphs
\setcounter{secnumdepth}{0} % Removes default section numbers
\title{
\vspace{1in}
\textmd{\textbf{Machine Learning Exam}} \\
\author{Christoffer Thrys√∏e - dfv107}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\pagenumbering{arabic}
\section{1. In a galaxy far, far away}
\subsection{Question 1.1}
The variance $\sigma^ 2_{\text{red}}$ of the redshifts detailed in the training data, was calculated using the following:
\begin{equation}
\sigma^2_{\text{red}} = \dfrac{1}{N} \sum\limits_{i=0}^N(X_i-\mu)^2
\end{equation}
where $\mu$ is the mean of the redshifts defined as:
\begin{equation}
\mu = \dfrac{1}{N} \sum\limits_{i=0}^N x_i
\end{equation}
The variance of the dataset \texttt{ML2016SpectroscopicRedshiftsTrain.dt} was measured to be:
$$
\sigma^ 2_{\text{red}} = 0.0106043856253
$$
The mean-squared-error (MSE) is defined as the following:
\begin{equation}
\text{MSE} = \dfrac{1}{N} \sum\limits_{i=0}^N (\hat{y_i} - y_i)^2 
\end{equation}
where $\hat{y}$ denotes the estimated value of the actual target value $y$. 
MSE has been applied on the SDSS predictions on the test data and the actual values. The returned MSE was the following:
$$
MSE = 0.000812319455494
$$
\subsection{Question 1.2}
Linear regression was performed by applying least squares, however the results were very unsatisfying, probably due to the linear dependency in the data. Therefore i have used the weights described in the announcement on Absalon. From these weights i have estimated the target value $\hat{Y}$ using the following:
\begin{equation}
\hat{Y_i} = wx_i+w_0
\end{equation}
The MSE on the training and test data is defined below:
\begin{align*}
\text{MSE}_{\text{training}} &=  0.0018621 \\
\text{MSE}_{\text{test}} &= 0.00186407
\end{align*}
Normalizing the error w.r.t the variance yields the following result:

\begin{align}
\dfrac{\text{MSE}_{\text{training}}}{\sigma^2_{\text{red}}} =
\dfrac{0.0018621}{0.0106043856253} = 0.17559716006
\label{eq:3}
\end{align}
As the least-squared estimator is unbiased, the mean-squared error is the variance of our linear estimator. By comparing the mean-squared-error, and the variance of the data we can see to what extend the estimator captures the variance of the data. For example if the variance of the data is high and the mean-squared-error is small compared, then the estimator has over-fitted the data. Equation \eqref{eq:3} will then be less than 1. If on the other hand the mean-squared-error is very large compared to the variance, then the estimator has under fitted the data. In this case equation \eqref{eq:3} will be greater than 1.
\subsection{Question 1.3}
For the non-linear regression method i have chosen the k-nearest neighbour. I have used the same implementation as in the previous assignment, however for regression instead of classification instead. The process of obtaining the k-nearest neighbours of a sample is the same, the euclidean distance from each point in question is taken to the rest of the points, the top k points with the lowest distance are marked as neighbours.The regression variant doesn't use a voting phase 
on the target values of the k-nearest neighbours, but the mean of their target values instead as shown in equation \eqref{eq:knn}.
\begin{equation}
\hat{y} =  \dfrac{1}{K} \sum\limits_{i=0}^K y(x_i)
\label{eq:knn}
\end{equation}
where $y$ denotes the target value associated with the independent variable $x$. \\
For hyper parameter tuning, i.e. finding the optimal value of $k$, i have used a 5-fold cross-validation. The training set of 5000 samples is split into 5 equally sized partitions, each of these partitions is by turn used as a validation set and the rest as a training set. The k-nearest neighbour algorithm is applied to each of the partitions, and the average MSE error over these folds determines the average error for the given value k. The k with the lowest average error is used as the parameter of the k-nearest neighbour algorithm.
\\
There was a number of reasons for choosing algorithm, ultimately the choice was between k-nearest-neighbours and neural networks. Both algorithms was considered as they usually make good generalizations on data, without making any assumptions about the data. As the dataset is quite big and consists of many features, this is wanted as finding trends in data, which addresses a certain algorithm is hard, due to the dimensionality. I tested my neural two-layer hidden network implementation and the result was sub par compared to the performance of  the k-nearest neighbour. There is although a set of shortcomings that follows the k-nearest neighbour. Although we may not need to make any assumptions on the data, we also assume that each feature contribute equally to the overall estimation, this is however rarely true. The computation time is also very high, when the dataset is big. One last thing which should be accounted for is that each data feature may measure on different scales, which results in certain features contributing more to the overall estimation. To counter this, the training data has been normalized to exhibit zero-mean and unit-variance. This affine mapping obtained by the training data, has also been applied to the training data. \\
The cross validation has been performed for the following values of k:
$$ k \in \left[ 1,3,5,7,9,11,13,15,17,19,21,23,25 \right] $$
The average error over the five folds is illustrated in figure 

\begin{figure}[H]
  \centering
  \includegraphics[width=17cm]{fig/1.png}
  \caption{}
  \label{fig:mulplots}
\end{figure}
The smallest MSE came from the parameter $k=9$, therefore 9 is the hyperparameter chosen. The following shows the result of running the 9-nearest-neighbour algorithm on the training and testing data. When applying k-nearest neighbour on the training data, the closest neighbour will always be itself, this is not the case for the test data, as the points are introduced to a dataset where they don't exists. The results are the following:
\begin{align*}
\text{MSE}_{\text{training}} &=  0.000465310841171 \\
\text{MSE}_{\text{test}} &=  0.000588914605838
\end{align*}
The above results both show an MSE, which is far lower than the liner regression model. Thus the non-linear 9-nearest-neighbour has a better performance %TODO ..
\section{2. Weed}
\subsection{Question 2.1}
For this assignment I have used the logistic regression algorithm implemented in previous assignments. The implementation uses the the steepest decent variant of gradient decent, where the gradient $g$ is updated as the following
\begin{equation}
g = - \dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{y_nx_n}{1+e^{y_nw^Tx_n}}
\end{equation}
and the weights are updated by: 
\begin{equation}
w = w - \epsilon g
\end{equation}
where $\epsilon$ is the learning rate of the gradient decent. The target class is evaluated as the following:
\begin{equation*}
h(x) = \theta(\mathbf{w}^T\mathbf{x})
\end{equation*}
where $\theta$ is the sigmoid function, which estimates the probability of the given class. Since we have a binary decision problem, we can define the decision boundary as the following: 
\begin{align*}
h(x) =
\begin{cases} 
      +1 & \text{for} \;\, h(x) \geq 0.5 \\
      -1 & \text{for} \;\, h(x) < 0.5
   \end{cases}
\end{align*}
For the model, i have used $\epsilon = 0.00001$. The small learning rate was used as values for the data was large, and a larger learning rate caused the gradient to grow large and cause overflow. The model has been trained using 5000 iterations of the steepest descend algorithm. \\
The zero-one-loss is reported on the training and testing data using the model trained using the training data. The zero-one-loss is defined as followed:
\begin{align*}
l(y,\hat{y}) =
\begin{cases} 
      0 & \text{if} \;\, y = \hat{y} \\
      1 & \text{otherwise}
   \end{cases}
\end{align*}
The errors on the training and test data are as followed:
\begin{align*}
l(y,\hat{y})_{training} &= 24\\
l(y,\hat{y})_{test} &= 18
\end{align*}
The empirical loss of the two errors is:
\begin{align*}
lL(y,\hat{y})_{training} &= 0.024\\
L(y,\hat{y})_{test} &= 0.0313
\end{align*}
\subsection{Question 2.2}



\end{document}
