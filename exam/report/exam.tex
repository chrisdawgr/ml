%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum}
\usepackage{tabularx}
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{
  backgroundcolor=\color{white},   % you must add \usepackage{color} or \usepackage{xcolor}
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1} % Line spacing

\pagestyle{fancy}
\lhead{ML exam - 2017} % Top left header
\chead{}
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule
\setlength\parindent{16pt} % Removes all indentation from paragraphs
\setcounter{secnumdepth}{0} % Removes default section numbers
\title{
\vspace{1in}
\textmd{\textbf{Machine Learning Exam}} \\
\author{Christoffer Thrysøe - dfv107 \\ Exam number: 347}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\pagenumbering{arabic}
\section{1. In a galaxy far, far away}
\subsection{Question 1.1}
The file \texttt{assignment1.py} implements the subroutines \texttt{get\_variance} and \texttt{MSE}, which outputs the\\below results \\
The variance $\sigma^ 2_{\text{red}}$ of the redshifts, detailed in the training data, was calculated using the following:
\begin{equation}
\sigma^2_{\text{red}} = \dfrac{1}{N} \sum\limits_{i=0}^N(X_i-\mu)^2
\end{equation}
where $\mu$ is the mean of the redshifts defined as:
\begin{equation}
\mu = \dfrac{1}{N} \sum\limits_{i=0}^N x_i
\end{equation}
The variance of the dataset \texttt{ML2016SpectroscopicRedshiftsTrain.dt} was measured to be:
$$
\sigma^ 2_{\text{red}} = 0.0106043856253
$$
The mean-squared-error (MSE) is defined as the following:
\begin{equation}
\text{MSE} = \dfrac{1}{N} \sum\limits_{i=0}^N (\hat{y_i} - y_i)^2 
\end{equation}
where $\hat{y}$ denotes the estimated value of the actual target value $y$. 
MSE has been applied on the SDSS predictions on the test data and the actual values. The returned MSE was the following:
$$
\text{MSE} = 0.000812319455494
$$
\subsection{Question 1.2}
The subroutine \texttt{getError} produces the below results
\\
Linear regression was performed by applying least squares, however the results were very unsatisfying, probably due to the linear dependency in the data. Therefore i have used the weights described in the announcement on Absalon. From these weights i have estimated the target value $\hat{y}$ using the following:
\begin{equation}
\hat{y_i} = wx_i+w_0
\end{equation}
The MSE on the training and test data is defined below:
\begin{align*}
\text{MSE}_{\text{train}} &=  0.0018621 \\
\text{MSE}_{\text{test}} &= 0.00186407
\end{align*}
Normalizing the error w.r.t the variance yields the following result:

\begin{align}
\label{eq:vars}
\dfrac{\text{MSE}_{\text{train}}}{\sigma^2_{\text{red}}} =
\dfrac{0.0018621}{0.0106043856253} = 0.17559716006
\end{align}
% variance we can expect
One can say that the MSE estimates the error variance. If we have that the points are distributed at random, the variance and the MSE of the linear fit will be close to identical, thus \eqref{eq:vars} will be 1. The only way we can have that \eqref{eq:vars} is greater than 1 is if the data is not linear as the otherwise the MSE will be less than the variance. If the data is perfectly linear and we apply linear regression, then the only way we can have that \eqref{eq:vars} is greater than 1 is if the linear fit is worse than the horizontal mean line. However this will not be possible when applying least squares, except if the data don't fit the model.

\subsection{Question 1.3}
The file \texttt{KNN.py} implements the k-nearest neighbour and cross-validation algorithms described in this section.
\\
For the non-linear regression method i have chosen the k-nearest neighbour. I have used the same implementation as in the previous weekly assignment, however for regression instead of classification.
I chose the k-nearest neighbour algorithm is that it usually makes good predictions on the data, without making any assumptions about the data. As the dataset is quite big and consists of many features, this is wanted as finding trends in data can be tough due to the dimensionality. There is although a set of shortcomings that follows the k-nearest neighbour. Although we may not need to make any assumptions on the data, we also assume that each feature contribute equally to the overall estimation, this is however rarely true. The computation time is also very high, when the dataset is big. One last thing which should be accounted for is that each data feature may measure on different scales, which results in features with high numerical ranges dominating low ones. To counter this, the training data has been normalized to exhibit zero-mean and unit-variance. This affine mapping obtained by the training data, has also been applied to the training data. \\
The process of obtaining the k-nearest neighbours of a sample is the same: The euclidean distance from each point in question is taken to the rest of the points, the top k points with the lowest distances are marked as neighbours.The regression variant doesn't use a voting phase 
on the target-values of the k-nearest neighbours, but the mean of their target-values instead, as shown in equation \eqref{eq:knn}.
\begin{equation}
\hat{y} =  \dfrac{1}{K} \sum\limits_{i=0}^K y(x_i)
\label{eq:knn}
\end{equation}
where $y$ denotes the target value associated with the independent variable $x$. \\
For hyper parameter tuning, i.e. finding the optimal value of $k$, i have used a 5-fold cross-validation. The training set of 5000 samples is split into 5 equally sized partitions, each of these partitions is by turn used as a validation set and the rest as a training set. The k-nearest neighbour algorithm is applied to each of the partitions, and the average MSE error over these folds determines the average error for the given value k. The k with the lowest average error is used as the parameter of the k-nearest neighbour algorithm.
\\

The cross validation has been performed for the following values of k:
$$ k \in \left[ 1,3,5,7,9,11,13,15,17,19,21,23,25 \right] $$
The average error over the five folds is illustrated in figure \ref{fig:mulplots}.
\begin{figure}[H]
  \centering
  \includegraphics[width=17cm]{fig/1.png}
  \caption{Average MSE error over 5 folds shown for increasing parameter k. The lowest average error is k=9}
  \label{fig:mulplots}
\end{figure}
The smallest MSE averaged over 5-folds was achieved using the parameter $k=9$, therefore 9 is the hyper parameter chosen. The following shows the result of running the 9-nearest-neighbour algorithm on the training and testing data. When applying k-nearest neighbour on the training data, the closest neighbour will always be itself, this is not the case for the test data, as the points are introduced to a dataset where they don't exists. The results are the following:
\begin{align*}
\text{MSE}_{\text{train}} &=  0.000465310841171 \\
\text{MSE}_{\text{test}} &=  0.000588914605838
\end{align*}
The above results both show an MSE, which is far lower than the liner regression model. Thus the non-linear 9-nearest-neighbour has a better performance, and provides a good generalization on the data.
\section{2. Weed}
\subsection{Question 2.1}
The file \texttt{logRes.py} implements the logistic regression algorithm described below.
\\
For this assignment I have used the logistic regression algorithm implemented in previous assignments. The implementation uses the the steepest decent variant of gradient decent, where the gradient $g$ is updated as the following
\begin{equation}
g = - \dfrac{1}{N} \sum\limits_{n=1}^N \dfrac{y_nx_n}{1+e^{y_nw^Tx_n}}
\end{equation}
and the weights are updated by: 
\begin{equation}
w = w - \epsilon g
\end{equation}
where $\epsilon$ is the learning rate of the gradient decent. The target class is evaluated as the following:
\begin{equation*}
h(x) = \theta(\mathbf{w}^T\mathbf{x})
\end{equation*}
where $\theta$ is the sigmoid function, which estimates the probability of the given class. Since we have a binary decision problem, we can define the decision boundary as the following: 
\begin{align*}
h(x) =
\begin{cases} 
      +1 & \text{for} \;\, h(x) \geq 0.5 \\
      -1 & \text{for} \;\, h(x) < 0.5
   \end{cases}
\end{align*}
For the model, i have used $\epsilon = 0.00001$. The small learning rate was used as values for the data was large, and a larger learning rate caused the gradient to grow large and cause overflow. The model has been trained using 5000 iterations of the steepest descend algorithm. \\
The zero-one-loss is reported on the training and testing data using the model trained using the training data. The zero-one-loss is defined as followed:
\begin{align*}
l(y,\hat{y}) =
\begin{cases} 
      0 & \text{if} \;\, y = \hat{y} \\
      1 & \text{otherwise}
   \end{cases}
\end{align*}
The errors on the training and test data are as followed:
\begin{align*}
l(y,\hat{y})_{train} &= 26\\
l(y,\hat{y})_{test} &= 20
\end{align*}
%The empirical loss of the two errors is:
%\begin{align*}
%L(y,\hat{y})_{training} &= 0.026\\
%L(y,\hat{y})_{test} &= 0.0348
%\end{align*}
where the training data consists of 1000 examples and the test data 574 examples. Although the above results may have a high error, the model exhibits good generalization on the test data as the empirical loss (average loss) is somewhat close to the expected loss.
\subsection{Question 2.2}
The file \texttt{svmc.py} implements the below experiments, grid search and cross-validation. \\
% https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine
We wish to determine G, which is the following quantity:
\begin{equation}
G = \left\lbrace \text{min}_{x_j,y_j) \in S \wedge y_i \neq y_j}
\left\lbrace || x_i - x_j || \right\rbrace | (x_i,y_i) \in S \right\rbrace 
\end{equation}
G is calculated by iterating over the training data, and for each point creating a list of training data points sorted by closest distance. The sorted list is then iterated, when a training point where $y_i \neq y_j$ is encountered, the distance $||x_i-x_j||$ is accumulated. The estimate $\sigma_{\text{Jaakkola}}$ is given by:
\begin{equation}
\sigma_{\text{Jaakkola}} = \text{median}(G)
\end{equation}
The bandwidth parameter $\gamma$ can be derived by the following:
\begin{align*}
\sigma_{\text{Jaakkola}} &= \sqrt{1 / (2\gamma_{\text{Jaakkola}})} \Rightarrow \\
\gamma_{\text{Jaakkola}} &= 1/2\sigma^2_{\text{Jaakkola}}
\end{align*}
I estimated $\sigma_{\text{Jaakkola}} = 609.035300487$
therefore $\gamma_{\text{Jaakkola}}$ is estimated by:
\begin{equation}
\gamma_{\text{Jaakkola}} = 1 / 2 \times 609.035300487^2 = 1.3479 \times 10^{-06}
\end{equation}
For the task, the library LIBSVM for python was used for training and classification. The library implements the desired C-SVM. The C parameter determines the softness of the margin, where a small C gives a soft margin and a large C gives a hard margin. Thus a large C will try to classify as much data as possible correctly, yielding a smaller margin, where a soft margin will try to maximize the margin allowing misclassified variables. For the SVM the radial gaussian kernel is applied, which is defined as followed:
\begin{equation}
k(x,z) = exp(-\gamma || x - z ||^2)
\end{equation}
$\gamma$ is the width of the gaussian kernel, therefore a a small $\gamma$ results in large variance. For parameter tuning, grid search was applied, where each parameter setting was evaluated by performing 5-fold cross-validation. The choosing of parameters which yielded the lowest average zero - one loss over the 5-folds was inserted into the grid. The grid index with the lowest average zero - one loss, was determined as parameter settings. The values for $C$ and $\gamma$ were picked from the following set:
\begin{align*}
(C,\gamma) \in \left\{ b^{-3}, b^{-2}, b^{-1}, 1, b, b^{2}, b^{3},b^4,b^5,b^6 \right\} \times \left\{ \gamma_{Jaakkola} \cdot b^{i} | i \in \left\{ -3, -2, -1, 0, 1, 2, 3 \right\} \right\}
\end{align*}
where $b \in \lbrace 2, e, 10 \rbrace$.
Running the 5-fold cross validation the parameters for $C$ and $\gamma$ which gave the lowest average zero - one loss was:
\begin{align*}
C &= b^4 \\
\gamma &= \gamma_{Jaakkola} \cdot b^{-1}
\end{align*}
The lowest error on the training data was measured when b was 2 or $e$. The above parameters was obtained during training, below is the error of applying the SVM, with the above parameters on the training and test data.
\begin{align*}
L(y,\hat{y})_{\text{train}} &= 0.015 \\
L(y,\hat{y})_{\text{test}} &= 0.0348
\end{align*}
It is notable that the training error is very low compared to error achieved by the logistic regression algorithm. However the test error is very high compared to the training error, which means that the model does not generalize well on the unseen data, as the expected loss does not track the empirical loss.
\subsection{Question 2.3}
The normalization of both procedures is defined respectively in the files they were implemented in. \\
To achieve zero-mean and unit-variance, we apply a linear mapping from the input data to the normalized output data, that is of the form $R^d \rightarrow R^d$. Thus we wish to determine some normalization function $ f_{a,b}$:
\begin{equation}
X_{f_{a,b}}  = \lbrace a x_i+b | x_i \in X \rbrace
\end{equation}
such that $\mu(X_{f_{a,b}})=0$ and $Var(X_{f_{a,b}})=1$ \\
To achieve this, the mean of each input feature is found and subtracted. The mean is calculated by:
\begin{equation*}
\mu_\mathbf{X} = \dfrac{1}{N} \sum\limits^{N}_{i=1}x_i
\end{equation*}
 Next the variance is found of each feature. The variance is calculated by:
\begin{equation*}
Var(X) = \dfrac{1}{N} \sum\limits^{N}_{i=1}(x_i - \mu_\mathbf{X})^2
\end{equation*}
The standard deviation $\sigma$ is equal to the square root of the variance and is used to normalize the data. The normalization can then be expressed as the following:
\begin{equation}
X_{f_{a,b}} = \dfrac{1}{\sigma_\mathbf{X}}\mathbf{x} + \dfrac{- \mu_\mathbf{X}}{\sigma_\mathbf{X}} = \dfrac{\mathbf{x} - \mu_\mathbf{x} }{\sigma_\mathbf{X}}
\label{norm}
\end{equation}
For the below algorithms i will first state how i predict the normalization will effect the estimation, then results of the algorithms on the normalized data.
\subsubsection{Logistic regression}
First i will determine how the estimator is affected by the normalization. Given a logistic regression applied to normalized data $ x'$ we include the affine transformation in the calculation:
\begin{align*}
w^Tx' &= w^T \cdot (a x_i +b) \\
&= w'^Tx
\end{align*}
when $x_0 = w_0 + \sum_{i=0}^d b_i w_i$ and $ w'_i = a w_i$. Therefore the weights obtained using normalized data, can be obtained by using the affine transformation parameters on the weights obtained from performing logistic regression without normalization. Thus the normalization shouldn't effect the classification procedure. \\
Logistic regression uses gradient decent and this procedure however effects from data normalization. The normalization can be said to normalize the gradient surface. Normalization is often  used in the context of optimization problems, where computations of large gradients are expensive and may cause overflow. The normalization also makes convergence faster and ensures that weights are updated somewhat equally. Given the above reasoning for normalizatio,n before applying logistic regression, I predict that the result will be somewhat equal to using the non-normalized data. Since the gradients have been normalized, I will have to adjust the learning rate $\epsilon$ and number of iterations. \\
Applying normalization to the data before performing logistic regression worsens the generalization in contrast to the SMV. Below is the zero - one loss when normalization is applied to the dataset:
\begin{align*}
l(y,\hat{y})_{train} &= 57\\
l(y,\hat{y})_{test} &= 44
\end{align*}
%The empirical loss of the two errors is:
%\begin{align*}
%L(y,\hat{y})_{\text{trainnorm}} &= 0.057 \\
%L(y,\hat{y})_{\text{testnorm}} &= 0.076
%\end{align*}
As evident from the above errors, the logistic regression estimator performs worse when applied to normalized data. This was not expected given the above discussion.
\subsubsection{Support vector machines}
Achieving zero-mean and unit-variance when using support vector machines is useful in multiple ways. Kernels often depend on distances between points e.g the Gaussian kernel. To avoid larger numeric ranges dominating small ones, it's necessary to scale the features to the same range. Like with logistic regression, SVM's solve an optimization problem, which benefits from scaled features. Due to the feature scaling, i predict that the error will be lower, when using normalized data.
%
%http://sebastianraschka.com/Articles/2014_about_feature_scaling.html
%The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Another advantage is to avoid numerical diculties during the calculation. Because kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial ker- nel, large attribute values might cause numerical problems. We recommend linearly scaling each attribute to the range [-1,+1] or [0,1].

Applying zero mean and unit variance to the dataset before running the training the support vector machine leads to a different loss and different choice of hyper parameters. $b=2$ is still the best setting for b. The found parameters using grid search are as followed:
\begin{align*}
C &= b^5 \\
\gamma &= \gamma_{Jaakkola} \cdot b^{-3}
\end{align*}
The linear mapping was achieved on the training set, the same mapping is used on test set. The training and test error is as followed:
\begin{align*}
L(y,\hat{y})_{\text{trainnorm}} &= 0.015 \\
L(y,\hat{y})_{\text{testnorm}} &= 0.0278
\end{align*}
We note from the above errors that the normalized data yields the same error on the training set, however as expected, we achieve a much better generalization of the data, as the test error is lower.
%TODO discussion on why
\subsubsection{Random Forest}
Intuitively there is no need for normalization as the recursive splitting is not determined by the magnitude of the features, but by an impurity measure. The threshold at each node is feature scale invariant, and a scaling of the features will only reflect in a shift of this threshold, thus not impacting the split. \\
For random forest we split the data, maximizing $G_{d,\theta}(S)$ the information gain. If we consider a split, on a data point $x$ normalized using the affine transformation  $f_{a,b}(x) = x'$. The split is determined by $(d,\theta)=(d,x'_d)$, the following is achieved:
\begin{equation}
\theta = x'_d = f_{a,b}(x)_d = ax_d + b_d \Leftrightarrow
x_d = (\theta -b_j)/a
\end{equation}
Thus the parameters of the affine transformation, uniquely defines the threshold, and therefore have no influence on the classification.
\subsection{Question 2.4}
The file \texttt{pca.py} implements the principle component analysis described below \\
The implementation of the PCA algorithm is located in the file \texttt{pca.py}. The algorithm performs the following steps:
\begin{enumerate}
\item{Separate features and target values of each data set}
\item{For each feature, calculate the mean and withdraw it from the data points}
\item{Compute the covariance matrix of the mean-less data}
\item{Compute eigenvalues and eigenvectors of the covariance matrix}
\item{Sort the eigenvectors based on the corresponding eigenvalue from largest to smallest}
\item{Take the top k (dimension to reduce to) eigenvectors. These correspond to the k principle components}
\item{Project the data into k dimensions by multiplying the principle components and the mean-less data}
\end{enumerate}
\subsubsection{Eigenspectrum}
To compute the eigenspectrum the eigenvalues have been listed in descending order, and plotted on a logarithmic scale as shown in figure \ref{fig:eig}.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{fig/2.png}
  \caption{Variance of each principle component in descending order. 90\% of the variance is described after the second principle component.}
  \label{fig:eig}
\end{figure}
The number of principle components necessary to explain 90 $\%$ of the variance was determined to be \textbf{2 components}. \\
From figure \ref{fig:eig} it is evident that the variance drops linearly until about the 9th component where the variance plummets.
\subsubsection{Projection onto two dimensions}
The data has been projected down to two dimensions, by performing PCA with k = 2, that is multiplying the first two principle components onto the mean-less data. Figure \ref{fig:scatter} shows a scatter plot for the for crops and weed.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{fig/3.png}
  \caption{Scatter plot of the data projected onto the two first principal components.}
  \label{fig:scatter}
\end{figure}
The below table shows the correlation between the classification and the colour which they were plotted in. :
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|}
  \hline
    \textsf{Class} & \textsf{Colour}\\
    \hline
    \textsl{Weed} & \textsl{Red} \\
    \hline
    \textsl{Crop} & \textsl{Green} \\
    \hline
  \end{tabular}
\end{table}
From figure \ref{fig:scatter} we can see that the projection onto the two first principal components, describes the data well, as the data is somewhat divided into two clusters. Many of the points are well separated, however some red points (weed) lie in the green (crop) cluster, which could cause misclassification of crop weed images as weed-less crop images.
\subsection{Question 2.5}
The implementation of k-means clustering is included in the file \texttt{kmean.py}. \\ The algorithm works as followed:
\begin{enumerate}
\item{$k$ cluster points are initialized, with a given position in the data. For this assignment the data is initialized to have the same position as the first two data points.}
\item{Each data point is assigned the cluster with the smallest euclidean distance.}
\item{For each cluster, the mean of the assigned points are calculated
and the cluster position is then updated with the mean.
}
\item{The two steps above are repeated until the clusters no longer move. The positions of these clusters are then returned.}
\end{enumerate}
Figure \ref{fig:clust1} shows the cluster position projected onto the two first principle components, using the same projection calculation as assignment 3.2. The starting positions of the two centroids correspond to the first two data points. All clusters converged after 30 iterations.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{fig/4.png}
  \caption{The result of running 4-clustering on the crop training dataset, where the position of the clusters (blue dots) have been projected onto the first two principle components.
  }
  \label{fig:clust1}
\end{figure}
I also tried to perform the clustering with random data points as initialization of the centroids instead of the first two. However every initialization seemed to result in the centroids being located in the same spot, although with different number of iterations before converging. I believe i got meaningful clusters, as the centroids lie in each cluster, however it's clear that the overlapping data points could cause a high degree of misclassification. The clusters of points lie very close to each other. A clearer division of the clusters would lead to better placements of the centroids.
\section{3. Generalization bound for learning with multiple feature mappings}
\subsection{Question 1}
First we note that the VC-dimension of a hyperplane in dimension $d$ is $d+1$ (Abu-Mostafa et al., 2012 page 52), that is we can shatter at most $d+1$ points. 
When performing a mapping from an $x$ space to a $z$ feature space, we map a $d$ dimensional vector, representing our input to a new $\tilde{d}$ feature space:
$$
X = (x_0,x_1,...,x_d) \xrightarrow{\text{  \hspace{0.15cm}} \Phi \text{  \hspace{0.15cm}}} z = (z_0,z_1,...,z_{\tilde{d} })
$$
The Q-th order polynomial transformation $\Phi_Q(x)$ is used, listing all monomials of degree zero up to Q. The dimension of $\tilde{d}$ is given by the number of monomials from the Q-th order polynomial transformation. This number is defined as:
\begin{equation}
\tilde{d} = \binom{d+Q}{Q}-1
\end{equation}
The minus one is because we don't count 1 as a dimension.
We now have a $\tilde{d}$ dimensional feature space. The total number that the hyperplane can shatter in this feature space and the VC-dimension is therefore:
%TODO PROOF http://www.cs.cmu.edu/~ninamf/ML11/lect1020.pdf
\begin{equation}
d_{vc} \leq  \tilde{d} + 1
\end{equation}
%We can prove this by induction over d and Q. For $n=1$, we correctly have the value $d+1$ and vice versa.
We now have the inequality for the wanted bound:
\begin{equation}
\left( \binom{d+Q}{Q}-1 \right) + 1 \leq (Q+1)d^Q
\end{equation}
which holds for all $Q,d \geq 0$
\subsection{Question 2}
From theorem 3.7 in the lecture notes we have the following VC generalization bound.
\begin{equation}
Pr \left[ \exists h \in \mathcal{H}: L(h) \geq L(\hat{h},S)+
\sqrt{ \dfrac{8 \text{ ln} (2((2n)^{d_{vc}}+1) / \delta}{n}} \right] \leq \delta 
\end{equation}
which we can write the complement of for all $\mathcal{H}$ to arrive at the following high probability bound on $L(h)$.
\begin{equation}
Pr \left[ \forall h \in \mathcal{H}: L(h) \leq L(\hat{h},S)+
\sqrt{ \dfrac{8 \text{ ln} (2((2n)^{d_{vc}}+1) / \delta}{n}} \right] \geq 1- \delta 
\end{equation}
which we can write in terms of the bounded vc dimension, where $d_{vc} \leq (Q+t)d^Q$
\begin{equation}
Pr \left[ \forall h \in \mathcal{H}: L(h) \leq L(\hat{h},S)+
\sqrt{ \dfrac{8 \text{ ln} (2((2n)^{(Q+t)d^Q}+1) / \delta}{n}} \right] \geq 1- \delta 
\end{equation}
\subsection{Question 3}
The norm for an n-dimensional vector is defined as followed (Lecture slides on kernels):
$$
|| x || = \sqrt{x_1^2 + ... + x_n^2}
$$
We can bound $||\Phi_Q(x)||$ by $||\Phi^+_Q(x)||$ as $\Phi^+_Q(x)$ will contain duplicate monomials of $\Phi_Q(x)$ and as each individual monomial norm will be positive we can write $||\Phi_Q(x)|| \leq ||\Phi^+_Q(x)||$. We note that we can write the norm of a vector in terms of its inner product:
\begin{equation}
||x|| = \sqrt{\langle x, x \rangle}
\end{equation}
Therefore we can write the following for the norm of $\Phi^+_Q(x)$:
\begin{equation}
||\Phi^+_Q(x)|| = \sqrt{ \langle \Phi^+_Q(x), \Phi^+_Q(x) \rangle}
\end{equation}
Using the kernel trick we can write the above using the $x$ space:
\begin{equation}
||\Phi^+_Q(x)|| = \sqrt{1+ \sum\limits_{i=1}^Q (x^Tx)^i}
\end{equation}
Thus we can conclude the following bound on $||\Phi_Q(x)||$:
\begin{equation}
||\Phi_Q(x)|| \leq \sqrt{1+ \sum\limits_{i=1}^Q (x^Tx)^i}
\end{equation}
For each monomial at some order we have the the norm of the monomial cannot exceed 1 as the monomial is some multiplication combination of $x$ and since $||x|| \leq 1$ we can say the same for the rest of the monomials. Thus if the number of monomials is $|\Phi_Q|$ then the norm is bounded as:
\begin{equation}
||\Phi_Q|| \leq \sqrt{| \Phi_Q|}
\end{equation}
%TODO  sqrt(ØQ)
\subsection{Question 4}
From theorem 3.8 in the lecture notes we have the following bound on the VC dimension for hypothesis with the width of the margin being at least $\rho$:
\begin{equation}
d_{vc}(\mathcal{H}_\rho) = \lceil R^2 / \rho^2 \rceil
\end{equation}
Where $R$ is the maximal norm. If we set $R \leq || \Phi_Q(x)||$ which was derived in question 3, we get the following:
\begin{equation}
d_{vc}(\mathcal{H}_\rho) \leq \lceil \sqrt{| \Phi_Q|}^2  / \rho^2 \rceil
\end{equation}
following the definition from the proof of the lecture notes, we set $
| \Phi_Q| / \rho^2 = i$, which we can use to express $w$: $\lceil || w^2|| \rceil \cdot| \Phi_Q|$. We can therefore express the following:
\begin{equation}
Pr \left[ \exists h \in \mathcal{H}_i: L(h) \geq L(\hat{h},S)+
\sqrt{ \dfrac{8 \text{ ln} (2((2n)^{\lceil || w^2|| \rceil \cdot| \Phi_Q|}+1) / \delta_i}{n}} \right] \leq \delta_i 
\end{equation}
the same values of $\delta$ can be used here so we get the same derivation as the proof of theorem 3.9 we can therefore achieve the following generalization bound.
\begin{equation}
Pr \left[ \exists h \in \mathcal{H}: L(h) \geq L(\hat{h},S)+
\sqrt{ \dfrac{8 \text{ ln} (2((2n)^{\lceil || w^2|| \rceil \cdot| \Phi_Q|}+1) (1+ \lceil || w^2|| \rceil \cdot| \Phi_Q|) \lceil || w^2|| \rceil \cdot| \Phi_Q|  / \delta}{n}} \right] \leq \delta
\end{equation}
%in which we can derive a high probability bound for L as followed:
\subsection{Question 5}
Given the infinite hypothesis set $\mathcal{H} = \cup_{Q=1}^\infty \mathcal{H}_Q$ we note from assignment 1 that it is possible to shatter $d+1$ points when the feature space has dimension d. Our hypothesis set now consist of all monomials  up to infinity and the feature space will therefore have dimension $R^\infty$. Therefore by definition we can shatter all $N$ points (represent all $2^N$ dichotomies), thus the VC dimension is:
$$ d_{vc}(\mathcal{H}) = \infty $$
\subsection{Question 6}
From the bound obtained in question 5, we can re-write it for all $w \in \mathcal{H}$:
\begin{equation}
Pr \left[ \forall h \in \mathcal{H}: L(h) \leq L(\hat{h},S)+
\sqrt{ \dfrac{8 \text{ ln} (2((2n)^{\lceil || w^2|| \rceil \cdot| \Phi_Q|}+1) (1+ \lceil || w^2|| \rceil \cdot| \Phi_Q|) \lceil || w^2|| \rceil \cdot| \Phi_Q|  / \delta}{n}} \right] \geq 1- \delta 
\end{equation} 
%From Question 5 we have that the VC-dimension is infinite, therefore we can shatter all $N$ points and define the growth function as followed:
%\begin{equation}
%m_\mathcal{H} = 2^N
%\end{equation}
%We can enter this into the VC-generalization bound (Abu-Mostafa et al., 2012 Theorem 2.5) and achieve the following bound
%\begin{equation}
%Pr \left[ \forall h \in \mathcal{H}: L(h) \geq \hat{L}(h,S) +
%\sqrt{\dfrac{8}{N}\text{ln} \dfrac{4m 2^{2n}}{\delta}} \right] \geq %1-\delta
%\end{equation}
%which is the desired bound, however not dependent on the margin.
\subsection{Question 7}
Not solved.
\subsection{Question 8}
From question 6, we note that even if the margin is large, the generalization will be bad if the hypothesis has a large VC dimension, as we multiply with the number of monomials. The intuitive notion behind this is that we can't map our input space to an infinite feature space and still have the property that maximizing the margin will provide a good generalization. Thus it follows from corollary 3.11 that when we have an infinite VC dimension, it is impossible to achieve a high probability bound, as $n$ goes to infinity. 
\subsection{Question 9}
%TODO error bar
Minimizing the bound leads to a greater generalization of the model, however the flexibility of the model is limited, resulting in a bigger error. Maximizing the margin (minimizing $||w||$) leads to a better generalization of the data, as it ensures that unseen data are more likely to be classified correctly. There is a trade-off when minimizing the generalization bound, as minimizing means to limit the size of the feature space of the given hypothesis. While achieving a good generalization, the limited model may fail to capture important parts of the data which a more complex hypothesis could have, thus resulting in a bad model. 
\end{document}